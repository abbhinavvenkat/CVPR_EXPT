<html>
<head>
<title>
</title>


<link rel="stylesheet" type="text/css" href="http://fonts.googleapis.com/css?family=Julius+Sans+One">
<link rel="stylesheet" type="text/css" href="http://fonts.googleapis.com/css?family=Lobster">
<link rel="stylesheet" type="text/css" href="http://fonts.googleapis.com/css?family=Lobster+Two">
<link rel="stylesheet" type="text/css" href="http://fonts.googleapis.com/css?family=Indie Flower">
<link rel="stylesheet" type="text/css" href="http://fonts.googleapis.com/css?family=Yanone+Kaffeesatz">
<style type="text/css">

/* standard css to maintain a fluid header - content - footer */

body, html {
	height:100%;
}

* {
	margin:0;
	padding:0;
}

body{
	background-color: #A8AFC7;
}

#container{
	min-height:100%;
}

#content{
	
	padding-bottom: 25em;
	margin-top: -2em;
	width: 80%;
	margin-left: auto;
	margin-right: auto;
}

h4 {
	color: #2D67A3;
	margin-top: 4em;
	margin-left: 18em;
	font-weight: bold;
}

.header-img a{
	text-decoration: none !important;;
	color: #767683;
}



.header-img{
	position: relative;
	margin: 0 auto;
	padding: 1.5em 1em;
	text-align: center;
	background-color: #080817;
	color: #fff;
	height: 5em;
	-webkit-box-shadow: -1px 45px 76px -32px rgba(0,0,0,1);
	-moz-box-shadow: -1px 45px 76px -32px rgba(0,0,0,1);
	box-shadow: -1px 45px 76px -32px rgba(0,0,0,1);
}

.header-img h2{
	font-size: 2em;
	letter-spacing: 2px;
	color: #fff;
	-webkit-transition: color 0.2s ease-out 0.2s;
    transition: color 0.2s ease-out 0.2s;
    -o-transition: color 0.2s ease-out 0.2s;
    transition: color 0.2s ease-out 0.2s;
    cursor:pointer;
    font-family: 'Lobster', serif;
    margin-top: -2px;
}

.title-txt h1{
	font-family: 'Yanone Kaffeesatz', serif;
	color: #2D67A3;
	font-weight: 900;
	padding-left: 8em;
	padding-top: 0.5em;
	font-size: 4em;
}

.title-txt-cnt h1{
	font-family: 'Yanone Kaffeesatz', serif;
	color: #2D67A3;
	font-weight: 900;
	padding-left: 5em;
	padding-top: 0.5em;
	font-size: 4em;
}

.title-txt-cnt1 h1{
	font-family: 'Yanone Kaffeesatz', serif;
	color: #2D67A3;
	font-weight: 900;
	padding-left: 6.5em;
	padding-top: 0.5em;
	font-size: 4em;
}

.control1{
	width:50em;
}

.label1{
	color: #fff;
}

.nav-bar{
	padding-left: 0em;
	margin-top: 2em;
}

.nav-bar form{
	margin-left: auto;
	padding-left: 14em;
	margin-top: 2em;
	color:#2D67A3;
}

.control{
	width:70em;
}

#footer{
	position:relative;
    bottom:0;
    width:100%;
    height:5em;
	background-color: #1A1A20;
	color: #fff;
	-webkit-box-shadow: inset 0px 45px 44px -11px rgba(0,0,0,0.75);
	-moz-box-shadow: inset 0px 45px 44px -11px rgba(0,0,0,0.75);
	box-shadow: inset 0px 45px 44px -11px rgba(0,0,0,0.75);
	clear:both; 
	margin-top:-5em;
}

.error-handle{
	border-radius: 5px;
	padding-left: 10px;
	padding-top: 10px;
	padding-bottom: 10px;
	background: #E68080;
	color: #660000;
	border-color: #660000;
	border: 2px solid;
	font-weight: bold;
}

.success-handle{
	border-radius: 5px;
	padding-left: 2.5em;
	padding-top: 10px;
	padding-bottom: 10px;
	background: #75C375;
	color: #004A00;
	border-color:#004A00;
	border: 2px solid;
	font-weight: bold;
	
}

.success-2{
	width: 24em;
	margin-left: auto;
	margin-right: auto;
	margin-top: 2.5em;
	margin-bottom: -2em;
}

.error-1{
	width: 22em;
	margin-left: auto;
	margin-right: auto;
	margin-top: 2.5em;
	margin-bottom: -2em;
}

.error-2{
	width: 24em;
	margin-left: auto;
	margin-right: auto;
	margin-top: 2.5em;
	margin-bottom: -2em;
}

.error-3{
	width: 15em;
	padding-left: 3.3em;
	margin-left: auto;
	margin-right: auto;
	margin-top: 2.5em;
	margin-bottom: -2em;
}

.error-4{
	width: 20em;
	padding-left: 3.3em;
	margin-left: auto;
	margin-right: auto;
	margin-top: 2.5em;
	margin-bottom: -3em;
}

.error-5{
	width: 15em;
	padding-left: 3.3em;
	margin-left: auto;
	margin-right: auto;
	margin-top: 2em;
	margin-bottom: 0em;
}


.error-6{
	width: 28em;
	padding-left: 3.3em;
	margin-left: auto;
	margin-right: auto;
	margin-top: 2em;
	margin-bottom: 0em;
}

.error-7{
	width: 24em;
	padding-left: 3.3em;
	margin-left: auto;
	margin-right: auto;
	margin-top: 2em;
	margin-bottom: 0em;
}

.success-4{
	width: 20em;
	margin-left: auto;
	margin-right: auto;
	margin-top: 2.5em;
	margin-bottom: 0em;
}

.success-1{
	width: 15em;
	margin-left: auto;
	margin-right: auto;
	margin-top: 2.5em;
	margin-bottom: -2em;
}

.function-board{
	background: #A8AFC7;
	padding-left: 15em;
	padding-top: 20px;
	padding-bottom: 20px;
	width:70em;
	margin-left: auto;
	margin-right: auto;
	border-radius: 5px;
	-webkit-box-shadow: -4px 2px 54px -18px rgba(0,0,0,0.75);
-moz-box-shadow: -4px 2px 54px -18px rgba(0,0,0,0.75);
box-shadow: -4px 2px 54px -18px rgba(0,0,0,0.75);
}

select{
	height: 2.4em;
	width: 10em;
	border: 1px solid #ccc;
	border-radius: 4px;
	color: #555;
}

h5{
	font-family: 'Lobster Two';
	padding-left: 74%;
	padding-top:2em;
}

.oops{
font-family: 'Julius Sans One';
font-size: 6.5em;
color: #CC0000;
padding-top: 1em;
padding-left: 1.2em;
font-weight: bold;
}


.table-fill {
  background: white;
  border-radius:3px;
  border-collapse: collapse;
  margin-left: -8.3em;
  max-width: 80em;
  padding:10px;
  width: 100%;
  box-shadow: 0 5px 10px rgba(0, 0, 0, 0.1);
  animation: float 5s infinite;
  margin-top: 5em;
}
 
th {
  color:#fff;;
  background:#2D67A3;
  border-bottom:4px solid #B8B8C0;
  border-right: 1px solid #B8B8C0;
  font-size:23px;
  font-weight: 100;
  padding:10px;
  padding-left: 30px;
  padding-right: 30px;
  text-align:left;
  text-shadow: 0 1px 1px rgba(0, 0, 0, 0.1);
  vertical-align:middle;
  font-family: 'Yanone Kaffeesatz', serif;
}

th:first-child {
  border-top-left-radius:3px;
  padding-right: 40px;
  padding-left: 40px;
}
 
th:last-child {
  border-top-right-radius:3px;
  border-right:none;
}
  
tr {
  border-top: 1px solid #B8B8C0;
  border-bottom-: 1px solid #B8B8C0;
  color:#666B85;
  font-size:16px;
  font-weight:normal;
  text-shadow: 0 1px 1px rgba(256, 256, 256, 0.1);
}
 
tr:hover td {
  background:#4E5066;
  color:#FFFFFF;
  border-top: 1px solid #B8B8C0;
  border-bottom: 1px solid #B8B8C0;
}
 
tr:first-child {
  border-top:none;
}

tr:last-child {
  border-bottom:none;
}
 
tr:nth-child(odd) td {
  background:#EBEBEB;
}
 
tr:nth-child(odd):hover td {
  background:#4E5066;
}

tr:last-child td:first-child {
  border-bottom-left-radius:3px;
}
 
tr:last-child td:last-child {
  border-bottom-right-radius:3px;
}
 
td {
  background:#FFFFFF;
  padding:5px;
  padding-left: 20px;
  text-align:left;
  vertical-align:middle;
  font-weight:300;
  font-size:15px;
  text-shadow: -1px -1px 1px rgba(0, 0, 0, 0.1);
  border-right: 1px solid #B8B8C0;
}

td:last-child {
  border-right: 0px;
}

th.text-left {
  text-align: left;
}

th.text-center {
  text-align: center;
}

th.text-right {
  text-align: right;
}

td.text-left {
  text-align: left;
}

td.text-center {
  text-align: center;
}

td.text-right {
  text-align: right;
}

</style>
</head>
<body>
	
<div id="container">  

  <header class="header-img">
      <a href="#"><h2>Publication Journal</h2></a>
  </header>

  <div id="content">
    

   
    	 <table  class=table-fill >
  <thead>
  <tr>
  <th class=text-left >Title</th>
  <th class=text-left >Date of Publication</th>
  <th class=text-lefta >Author(s)</th>
  <th class=text-left >Research Group/University</th>
  <th class=text-left >No. of Citations</th>
  <th class=text-left >Conference/Journal</th>
  <th class=text-left >Summary</th>
  <th class=text-left >Softcopy Link</th>
  </tr>
  </thead>

  
    <tbody class=table-hover>

    <tr>
    <td class=text-left ><div style="width: 250px">Learning Efficient Point Cloud Generation for Dense 3D Object Reconstruction</div></td>
    <td class=text-left >21 June 2017</td>
    <td class=text-left >Chen-Hsuan Lin, Chen Kong, Simon Lucey</td>
    <td class=text-left >The Robotics Institute, CMU</td>
    <td class=text-left >1</td>
    <td class=text-left >AAAI</td>
    <td class=text-left >
    <div style="width: 450px">Learning volumetric pre-dictions using deep networks with 3D convolutional operation is computationally wasteful (since info is only on the surface).
    They propose a novel 3D generative modeling framework to efficiently generate object shapes in the form of dense point clouds. They use 2D convolutional operations to predict the 3D structure from multiple viewpoints and jointly apply geometric reasoning with 2D projection optimization. (Done for rigid objects)</div>
	</td>
    <td class=text-left ><a href="./Effecient_point_cloud_cmu.pdf">Here</a></td>
    </tr>
  

    <tr>
    <td class=text-left ><div style="width: 250px">SilNet : Single- and Multi-View Reconstruction by Learning from Silhouettes</div></td>
    <td class=text-left >21 Nov 2017</td>
    <td class=text-left >Olivia Wiles, Andrew Zisserman</td>
    <td class=text-left >Vision Geometry Group, University of Oxford</td>
    <td class=text-left >0</td>
    <td class=text-left >BMVC, 2017 (Best Poster Award)</td>
    <td class=text-left >
    <div style="width: 450px">(Encoder)->(Max Pool for Views)->(Upsample)->(Decoder)->3D Object  <br> <br>
    They introduce a deep-learning architecture trained on the loss function IoU, and handle multiple views in an order-agnostic manner. (Done for rigid objects)	
    </div>
	</td>
    <td class=text-left ><a href="./Silnet.pdf">Here</a></td>
    </tr>

    <tr>
    <td class=text-left ><div style="width: 250px">A Point Set Generation Network for 3D Object Reconstruction from a Single Image</div></td>
    <td class=text-left >7 Dec 2016</td>
    <td class=text-left >H Fan, Hao Su, Leonidas Guibas</td>
    <td class=text-left >Stanford University</td>
    <td class=text-left >26</td>
    <td class=text-left >CoRR</td>
    <td class=text-left >
    <div style="width: 450px"> They address the problem of 3D reconstruction from a single image, generating a straight-forward form of output â€“ point cloud coordinates. Their final  solution  is  a conditional shape sampler,  capable of predicting multiple plausible  3D  point  clouds  from  an  input  image.
    </div>
	</td>
    <td class=text-left ><a href="./point_set.pdf">Here</a></td>
    </tr>

	<tr>
    <td class=text-left ><div style="width: 250px">Object-Centric Photometric Bundle Adjustment with Deep Shape Prior</div></td>
    <td class=text-left >4 Nov 2017</td>
    <td class=text-left >Rui Zhu, Chaoyang Wang et al.</td>
    <td class=text-left >The Robotics Institute, Carnegie Mellon University</td>
    <td class=text-left >0</td>
    <td class=text-left >arXiv, 2017</td>
    <td class=text-left >
    <div style="width: 450px"> In this paper we make an effort to bring these two seemingly disparate strategies together (SfM and DL). We introduce learned shape prior in the form of deep shape generators into Photometric Bundle Adjustment (PBA) and propose to accommodate full 3D shape generated by the shape prior within the optimization-based inference framework.
    </div>
	</td>
    <td class=text-left ><a href="./photometric_bundle.pdf">Here</a></td>
    </tr>

	<tr>
    <td class=text-left ><div style="width: 250px">Scaling CNNs for High Resolution Volumetric Reconstruction from a Single Image</div></td>
    <td class=text-left ></td>
    <td class=text-left >Adrian Johnston et al.</td>
    <td class=text-left >University of Adelide</td>
    <td class=text-left> 0</td>
    <td class=text-left >ICCV, 2017</td>
    <td class=text-left >
    <div style="width: 450px"> State-of-the-art methods are able to reconstruct 3-D shapes represented by volumes of at most 32^3 voxels using state-of-the-art desktop computers. In this work, we present a scalable 2-D single view to 3-D volume reconstruction deep learning method, where the 3-D (deconvolution) decoder is replaced by a simple inverse discrete cosine transform (IDCT) decoder.
    </div>
	</td>
    <td class=text-left ><a href="./Johnston_Scaling_CNNs_for_ICCV_2017_paper.pdf">Here</a></td>
    </tr>

    <tr>
    <td class=text-left ><div style="width: 250px">Variational Autoencoders for Deforming 3D Mesh Models</div></td>
    <td class=text-left >13 Sep 2017</td>
    <td class=text-left >Qingyang Tan et al.</td>
    <td class=text-left >University of Chinese Academy of Sciences, Cardiff University et al.</td>
    <td class=text-left >0</td>
    <td class=text-left >arXiv, 2017</td>
    <td class=text-left >
    <div style="width: 450px"> In this paper, they study the problem of analyzing deforming 3D meshes using deep neural networks. They propose a novel framework which we call mesh variational autoencoders (mesh VAE), to explore the probabilistic latent space of 3D surfaces. This is a generative network that generative non-rigid objects such as hands, face, humans in different poses etc. 
    </div>
	</td>
    <td class=text-left ><a href="./mesh_vae.pdf">Here</a></td>
    </tr>

    <tr>
    <td class=text-left ><div style="width: 250px">SurfNet: Generating 3D shape surfaces using deep residual networks</div></td>
    <td class=text-left >12 March 2017</td>
    <td class=text-left >Ayan Sinha, Asim Unmesh et al.</td>
    <td class=text-left >MIT, IIT Kanpur et al.</td>
    <td class=text-left >5</td>
    <td class=text-left >arXiv, 2017</td>
    <td class=text-left >
    <div style="width: 450px">  Current 3D learning paradigms for predictive  and  generative  tasks  using  convolutional  neural  networks  focus  on  a  voxelized  representation  of  the  object. Lifting  convolution  operators  from  the  traditional  2D  to 3D results in high computational overhead with little additional benefit as most of the geometry information is contained on the surface boundary. Here we study the problem  of  directly  generating  the  3D  shape  surface  of  <strong>rigid and non-rigid</strong> shapes using deep convolutional neural networks. 
    </div>
	</td>
    <td class=text-left ><a href="./Surfnet.pdf">Here</a></td>
    </tr>

    <tr>
    <td class=text-left ><div style="width: 250px">3D Reconstruction of Simple Objects from A Single View Silhouette Image</div></td>
    <td class=text-left >17 Jan 2017</td>
    <td class=text-left >Xinhan Di et al.</td>
    <td class=text-left >Trinity College Ireland, NUS</td>
    <td class=text-left >1</td>
    <td class=text-left >CoRR, 2017</td>
    <td class=text-left >
    <div style="width: 450px"> In  this work,  we  propose  novel  stacked  hierarchical  networks and an end to end training strategy to tackle a more challenging task for the first time, 3D reconstruction from a single-view  2D  silhouette  image.     
    </div>
	</td>
    <td class=text-left ><a href="./Silhouette.pdf">Here</a></td>
    </tr>

	<tr>
    <td class=text-left ><div style="width: 250px">Compact Model Representation for 3D Reconstruction</div></td>
    <td class=text-left >23 June 2017</td>
    <td class=text-left >Jhony K. Pontes et al.</td>
    <td class=text-left >CMU, Queensland University of Technology.</td>
    <td class=text-left >0</td>
    <td class=text-left >CoRR, arXiv</td>
    <td class=text-left >
    <div style="width: 450px"> 3D reconstruction from 2D images is a central problem in computer vision.  It is well known however that only one image cannot provide enough information for such a reconstruction.  A prior knowledge that has been entertained are 3D CAD models due to its online ubiquity. We introduce an approach to compactly represent a 3D mesh. Our method first selects a 3D model from a graph structure by using a novel free-form deformation (FFD) 3D-2D registration, and then the selected 3D model is refined to best fit the image silhouette.
    </div>
	</td>
    <td class=text-left ><a href="./compact_model.pdf">Here</a></td>
    </tr>

	<tr>
    <td class=text-left ><div style="width: 250px">Learning Category-Specific Deformable 3D Models for Object Reconstruction</div></td>
    <td class=text-left >1 June 2016</td>
    <td class=text-left >Jitendra Malik et al.</td>
    <td class=text-left >University of California, Berkeley et al.</td>
    <td class=text-left >7</td>
    <td class=text-left >IEEE Transactions</td>
    <td class=text-left >
    <div style="width: 450px"> We address the problem of fully automatic object localization and reconstruction from a single image.  Here we leverage recent advances in learning convolutional networks for object detection and segmentation and introduce a complementary network for the task of camera viewpoint prediction. Our main contribution is a new class of deformable 3D models that can be robustly fitted to images based on noisy pose and silhouette estimates computed upstream and that can be learned directly from 2D annotations available in object detection datasets.
    </div>
	</td>
    <td class=text-left ><a href="./category.pdf">Here</a></td>
    </tr>

    <tr>
    <td class=text-left ><div style="width: 250px">Large-Scale 3D Shape Reconstruction and Segmentation from ShapeNet Core55</div></td>
    <td class=text-left >27 Oct 2017</td>
    <td class=text-left >Li Yi et al.</td>
    <td class=text-left >Stanford University, Princeton, Oxford, Facebook AI Research et al.</td>
    <td class=text-left >0</td>
    <td class=text-left >ICCV, 2017</td>
    <td class=text-left >
    <div style="width: 450px"> They perform two tasks: part-level segmentation of 3D shapes and 3D reconstruction from single view images. <br>
    	3D Reconstruction: 3D VAE + alpha-GAN
    </div>
	</td>
    <td class=text-left ><a href="./SHREC.pdf">Here</a></td>
    </tr>

    </tbody>
  		</table>

   
  </div>
</div>
  
<div id="footer"> 
  <h5>Created by Abbhinav Venkat</h5>
</div>

</body>
</html>
